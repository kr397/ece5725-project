<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>PiDog</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="style.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    
    <nav id="sidebar">
      <div class="sidebar-header">
          <h2>PiDog</h2>
          <img class="img-rounded" src="images/dog_normal.png" alt="Generic placeholder image" ></img>
          <h3>ECE 5725 Final Project</h3>
          <h4>Aryaa Pai | Krithik Ranjan </h4>
      </div>
      <ul class="list-unstyled components">
          <li >
              <a href="index.html">Home</a>
          </li>
          <li >
              <a href="goals.html">Project Objective</a>
          </li>
          <li>
            <a href="features.html">Features</a>  
          </li>
          <li class="active">
            <a href="implementation.html">Implementation</a>
          </li>
          <li>
              <a href="future_scope.html">Future Scope</a>
          </li>
          <li>
              <a href="conclusion.html">Conclusion</a>
          </li>
      </ul>
    </nav>

    
    <div class="container starter-templated">
        <div style="text-align:left; padding: 0px 30px;">
            <h2 style="text-align: center;">Implementation</h2>
            <p>The software system of the PiDog was divided into four sections: speech recognition, hand detection, motion, and animation. Such a modular organization of the codebase helped separate different functions that the robot performs, and also in the implementation and testing of the different algorithms. While debugging, we were able to run the modules separately and verify their behavior before integrating into the overall system. In terms of division of labor, both the team members were able to work on different modules after brainstorming together, making the design process more efficient and effective. To run the entire system, the four modules are run separately as different processes. Since there are four cores on the Raspberry Pi 4, we expected the Linux scheduler to assign each process to a different core, thus providing a significant performance benefit. Lastly, the Python implementation of each of these modules had different requirements (python2, python3, sudo user, etc.), and running them as different processes removed any such Python conflicts between them. While these are separate modules, they have constant back-and-forth communication between them to run the complete system. This communication has been implemented using named pipes or FIFOs. Overall, we have designed the system to be run either by separate execution of the different modules (which provides constant feedback on the terminal), or by directly executing the <code>main.sh</code> which automatically runs the modules in background.</p>

            <h3>Speech Recognition</h3>
            <p>The speech recognition module handles the detection and recognition of voice commands by the robot. This process runs continuously to record small snippets of audio through the USB microphone, and sends those as requests to the Google Speech Engine to detect and recognize any English words in the recording. Once a command is found, it is forwarded to the other modules to handle. This module is itself divided into four files: <code>record.py</code>, <code>speech.y</code>, <code>ping.py</code> and <code>main.py</code>, where the first three consist of helper functions and classes, and the last serves as the main executable script when the module is run.</p>
            <p>The <code>record.py</code> file consists of the <code>RecordAudio</code> class which completely handles recording of the audio snippets later used for recognition. To record audio from the USB microphone on the Raspberry Pi, we used <b>PyAudio</b> library. This serves as a Python interface for Linux’s PortAudio library. The class was implemented by referring to the PyAudio’s documentation and example code, and included two functions, one to record and the other to save the recording into a <code>.wav</code> file. The parameters we used for recording were a sampling rate of 44100 Hz, recorded in chunks of 1024 from a single channel (as we were not using a stereo microphone). After trial and error, we set a fixed recording duration of 3 seconds, which we felt was enough for any single-word command, also allowing buffer time to adjust for noise.</p>
            <p>The requests for speech recognition were handled in a function defined in <code>speech.py</code> using the <b>SpeechRecognition</b> library. This function simply took the audio file recorded earlier as input and sent a request to the Google Speech Engine through direct functions of SpeechRecognition. This library also contains functions to send requests to other speech recognition services, like IBM Watson, Microsoft Bing, PocketSphinx, etc., most of which either require a license or extra installation. The default recognition through Google mostly worked well, so we chose to keep that in our system. We had also tried to use SpeechRecognition’s record feature which automatically records until silence is detected and sends the audio as a request for speech recognition. We found that the recognition accuracy of this method was much less than using PyAudio to record, and it also didn’t have any way of controlling the duration of the recording. Therefore, we kept PyAudio as the method of recording.</p>
            <p>This module also consists of the <code>ping.py</code> script which is not used for speech recognition, but is required for the ‘Wake-up’ feature of PiDog. This feature is implemented using pings, which are small data packets sent over the network to another client. With this script, the Raspberry Pi pings the IP address of the user’s mobile phone in order to determine whether the phone is connected to the home network (and thus whether the user is at home or not). The script is designed to keep pinging the device until it receives a successful response, thus acting as a waiting loop for waking up the system.</p>
            <p>These different parts of the speech recognition module have been integrated into the <code>main.py</code> script. This main process runs in a continuous loop, repeatedly recognizing voice commands and passing them over to the rest of the system. It starts off with first running and waiting for the <code>ping.py</code> script until the user’s phone is detected on the network. Now, the robot has ‘woken up’ and the script indicates the animation module to produce the barking sound. Inside the continuous loop, the module uses the <code>RecordAudio</code> object to record an audio snippet, and then uses the <code>speech.py</code> function to send the request to Google speech engine. If nothing is recognized, it does so repeatedly until any words are found. Before and after each recording instance, the script also communicates to the animation module to display the ear-up and ear-down animation. Once a voice command is recognized, the script checks whether it is one of the commands that the system already recognizes, and accordingly conveys to both the animation and the hand-detector modules. For the initial set of voice commands, the module is equipped with a few common variations of each command to compare with, in order to account for inaccuracies in speech recognition. The script now waits for acknowledgment from both the animation, and the hand-detector modules. The acknowledgment might differ depending on the kind of command that was sent, which helps the module decide whether a new command has been identified correctly. If the <code>QUIT</code> command is detected, the script breaks out of its loop.</p>
            <p>In order to test and debug the speech recognition module independently from the other modules, we had first developed the <code>test_sr.py</code> script, which runs exactly like the main script except without any of the communications with other modules. It instead prints out the status and the expected behavior, which helped us ensure the functionality of this module before integration into the system. It was through repeated testing that we determined the common variations of the commands, as recognized by the system, which have been shown below.</p>
            
            <h3>Hand Detector: Computer Vision</h3>

            <h3>Hand Detector: Classification</h3>

            <h3>Motion</h3>
            <p>The motion module is responsible for handling all the physical movements of the PiDog robot. It runs in a continuous loop, waiting for commands from the hand-detector module and moves the robot accordingly. While moving, the module also repeatedly reads the ultrasonic sensor to determine if there is an obstacle in front of the robot and stop. Like the other modules, this has also been organized into different files with class definitions and a main script. The implementation of motor control is very similar to that of Lab 3, using GPIO and PWM on the Raspberry Pi.</p>
            <p>The <code>Motor</code> and <code>Robot</code> classes handle the interface with both the motors through GPIO. While the <code>Motor</code> class is for individual motors, the <code>Robot</code> class instantiates two motor objects and manipulates them together. They have been implemented as independent classes with just general functions to move the motors and/or the robot in different ways (forward, backward, stop, etc.). The <code>Ultrasonic</code> class in <code>sensor.py</code> serves to get on-demand distance data from the HC-SR04 ultrasonic distance sensor installed on the robot. In order to detect distance, the sensor is used to send a fixed duration sound pulse (10 us) and then measure the duration of the reflected pulse from an object in front. Half of this duration is multiplied by the speed of sound in air to determine the distance of the object with the following formula: <pre><code>distance = ( pulse_time * 34300 ) / 2   # Using the speed of sound as 34300 cm/s</code></pre></p>
            <p> In the <code>main.py</code> script of the motion module, the process runs continuously to receive commands and perform the appropriate motion. Inside the main loop, it first waits for the command from the hand-detector module. If the command is one of <code>GO</code>, <code>BACK</code>, <code>LEFT</code>, <code>RIGHT</code>, it performs the forward, rotate, left turn, and right turn motions respectively, else it stays stationery. Each of these motion states also have a predetermined time duration (e.g. 2 seconds for <code>GO</code>), for which the script waits after executing the motion, and then stops the robot. The module also constantly checks the distance sensor for any obstacles, and if an object is detected within 20cm while the robot is moving forward, the motion is interrupted. Finally, the module sends an acknowledgement to the hand-detector that the motion has been completed. The script only breaks out of the loop if the command it receives is <code>QUIT</code>.</p>


                
        </div>
    </div>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>